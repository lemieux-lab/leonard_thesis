# leonard_thesis
## Chapter 3 Développement du modèle des Factorized Embeddings
### Objectif
Factorized embeddings a été introduit par (Trofimov et al. 2020) et basé sur les modèles gene2vec de (Du et al. 2019) et de (Choy, Wong, and Chan 2018). 
Ce modèle affirme pouvoir capturer l’information biologique sous la forme de représentations internes utiles pour la complétion de tâches auxiliaires. Nous avons identifié deux lacunes majeures liées à ces propositions. 1) Les temps de calculs rapportés par la méthode proposée (72h pour 500 epochs, 1000 x 60,600 gènes, sur nvidia-1080-Ti.) sont très élevés. 2) La méthode telle que proposée ne permet pas l’inférence de nouveaux points, ce qui empêche le processus de validation croisée véritablement non-biaisée de tâches auxiliaires. 
Dans ce chapitre, j’explore ces deux problèmes et comment nous sommes parvenus à y trouver des solutions computationnelles. Pour cette recherche, le modèle des Factorized Embeddings a été recodé dans le langage Julia qui offre une meilleure puissance de calcul (JuliaLang.org). Dans un premier temps, nous rapportons les changements algorithmiques apportés à l’entrainement du réseau pour accélérer l’apprentissage en plus du changement de langage. Les résultats empiriques comparant les deux implémentations sont rapportés. Nous rapportons les pistes de modifications permettant de prochaines accélérations. Dans un second temps, nous montrons l’implémentation capable de faire l’inférence de nouveaux points via un modèle pré-entrainé. 
Enfin, nous montrons le potentiel de cette technique comme méthode de réduction dimensionnelle pour un problème simple. 
### Résultats
#### Hyper-paramètres: Paramètres influençant la qualité et les propriétés des embeddings générés.
Les hyperparamètres de L2 et la taille de l’embedding des gènes affecte la distribution de densité des échantillons dans l’espace embedding patient. Graphiques de densité 2D avec 30 x 30 bins exécutés avec le module CairoMakie de Julia. Axe X : dimension 1 de l’embedding patient. Axe Y : dimension 2 de l’embedding patient. Colonnes : modèles entrainés respectivement avec une L2 = 0, 1e-8, 1e-7, 1e-6 et 1e-5. Rangées : modèles entrainés avec des couches d’embedding des gènes de taille 10, 25, 50, 75, 100, 1000. Les modèles sont entrainés sur les données d’expression des gènes codants de TCGA (n=10344, d=19962) via SGD avec l’optimiseur ADAM avec 4 profils par mini-batch pour 100000 itérations via le module Flux de Julia. Le réseau de neurones artificiel a trois couches cachées de 100, 50 et 50 neurones et une MSE comme fonction de cout.

Les hyperparamètres de L2 et de la taille de l’embedding gènes affecte la qualité de la répartition des échantillons de TCGA par type de cancer. Axes-X : dimension 1 de l’embedding patient. Axes-Y : dimension 2 de l’embedding patient. Colonnes : modèles entrainés respectivement avec une L2 = 0, 1e-8, 1e-7, 1e-6 et 1e-5. Rangées : modèles entrainés avec des couches d’embedding des gènes de taille 10, 25, 50, 75, 100, 1000. Les 33 types de cancer sont représentés par les couleurs des points avec le module CairoMakie de Julia. Les modèles sont entrainés sur les données d’expression des gènes codants de TCGA (n=10344, d=19962) via SGD avec l’optimiseur ADAM avec 4 profils par mini-batch pour 100000 itérations via le module Flux de Julia. Le réseau de neurones artificiel a trois couches cachées de 100, 50 et 50 neurones et une MSE comme fonction de cout.